\chapter{Mise-en-bouche}

\todo{Avoid definitions at all cost.}

\todo{Highlight contributions.}

\todo{Roadmap link to later chapters.}

This thesis is a compilation of the contributions from four papers:
%
\begin{enumerate}
	\item[A] ``\emph{Solving \(k\)-SUM Using Few Linear Queries}''~\cite{CIO16}
	\item[B] ``\emph{Subquadratic Algorithms for Algebraic 3SUM}''~\cite{BCILOS19}
	\item[C] ``\emph{Subquadratic Encodings for Point Configurations}''~\cite{CCILO19}
	\item[D] ``\emph{Encoding 3SUM}''~\cite{CCILMO19}
\end{enumerate}
%
Paper A was presented at CG:YRF 16 and ESA 16.
%
Paper B was presented at EuroCG 17 and SoCG 17. It is published in DCG.
%
Paper C was presented at FWCG 17, EuroCG 18, and SoCG 18. It will appear in JoCG.
%
Paper D was presented at EuroCG 19.

We begin this thesis with an overview of the studied topic.
%
We explain the context in which those papers were written and expose
the contributions contained in each of them.

\paragraph{Degeneracy Testing Problems}
%
In this thesis,
we study \emph{Degeneracy Testing Problems}:
an instance of size \(n\) of such a problem is a single point in
high-dimensional euclidean space \(q \in \mathbb{R}^{O(n)}\). Such an instance
is called \emph{general} if and only if it passes a series of algebraic tests
(usually \(n^{O(1)}\) of them). If it fails one of the tests, it is
called \emph{degenerate}.
%
Our goal is to determine how fast an instance can be classified as general or
degenerate.

The terminology is justified because most instances
are general: the set of degenerate instances is a zero-measure subset of the
input space. It also makes sense to visualize the input space as the euclidean
space: the algebraic tests naturally induce a partition of
the input space into semialgebraic sets. Solving the problem therefore amounts
to locate the input point \(q\) in this partition of space. Our goal
is thus to determine how fast this input point can be located.

Degeneracy testing problems are easy decision problems because there are only
a finite number of candidate tests to try. The ones we study can all be solved
by brute-force in polynomial time because the number of tests is polynomial.
We show how, in some cases, this naive approach can be clearly subsumed by
divide and conquer techniques exploiting the geometry of the setting.

\paragraph{GPT}
Let us illustrate by giving a first example of a degeneracy testing problem. We
begin with a definition:

\begin{definition}
A set of \(n\) points in \(\mathbb{R}^d\)
is in general position if and only if every (\(d+1\))-subset spans the entire
space. A point set that is not in general position is called \emph{degenerate}.
\end{definition}

The General Position Testing problem (GPT) asks to decide if a given set of
\(n\) point is in general position. We can solve this problem by brute-force in
\(O(n^{d+1})\) time.
We can do it an order of magnitude faster
by constructing the dual arrangement of hyperplanes in
\(O(n^d)\) time.
Improving on this slightly better solution appears to be non-trivial: there
exists a class of algorithms that cannot do better even though they
exploit one of the core structures of the problem, the chirotope axioms.
On the other hand, information theory only gives a decision tree
lower bound of \(\Omega(n \log n)\).
A popular conjecture is that no \(o(n^d)\) time real-RAM
algorithm exists for this problem.

\paragraph{Nonuniform Algorithms}
Another model of computation in which no \(o(n^d)\) time algorithm for GPT
is known is the algebraic computation tree model. In this model,
an algorithm is a tree whose internal nodes are either arithmetic operations or
sign tests on real variables, and whose leaves are the result of the computation.

This model is
more generous than the real-RAM model in the sense that all computations that
can be carried out by only knowing the input size incur no cost.
%
Because a computation tree has a fixed size, we need a different tree for each
input size.
%
Therefore,
we say that this model is \emph{nonuniform} since it allows to have a
distinct algorithm for each input size.

This thesis considers both uniform algorithms in the real-RAM and word-RAM
models of computation and nonuniform algorithms in the algebraic computation
tree, algebraic decision tree, and linear decision tree models of computation.
For a given task, the complexity of the nonuniform algorithm is less than
the complexity of the uniform algorithm. While a nonuniform algorithm is rarely
practical, designing those at least means making progress on the question of
whether a sensible computation tree lower bound can be derived.

\paragraph{3SUM}
The 3SUM problem also falls in the category of degeneracy testing problems.
This problem asks to decide whether a given set of \(n\) numbers contains a triple
whose sum is zero. We can solve this problem by brute-force in \(O(n^3)\) time,
and in \(O(n^2)\) time with a slightly more clever algorithm.

However toyish 3SUM may look like, it is considered one of several key problems
in P: many geometric problems reduce from it in subquadratic time. Hence, a
conjectured quadratic lower bound on 3SUM implies a conditional lower bound on
all those more practical problems.

Like for GPT, there exist lower bounds for 3SUM in restricted models of
computation: 3SUM cannot be decided in \(o(n^2)\) time if the only way we
inspect the input is by testing for the sign of weighted sums of three
input numbers.

Even before this lower bound was known, it was conjectured that a quadratic lower
bound would hold in other models of computation like the real-RAM model.

A first stab at the conjecture was made when it was proven that for integer
input numbers, it is possible to beat the conjectured lower bound by a few
logarithmic factors~\cite{BDP08}.
However, it remained open whether such improvements were
possible for real inputs.

Eventually, in a breakthrough paper, Gr\o nlund and Pettie gave a subquadratic
uniform algorithm that shaves a root of a logarithmic factor from quadratic
time~\cite{GP18}.
%
Since then more roots of logarithmic factors have been shaved~\cite{Fr15,GS15}.
%
To this day, it is still conjectured that, for all \(\delta > 0\), 3SUM
requires \(\Omega(n^{2 - \delta})\) time to solve in the real-RAM model.

\paragraph{\(k\)-SUM}

The paper of Gr\o nlund and Pettie also discusses the following generalization of
the 3SUM problem: ``For a fixed \(k\), given a set of \(n\) real numbers,
decide whether there exists a \(k\)-subset whose elements sum to zero.''
This problem is called the \(k\)-SUM problem.

Obviously, the 3SUM problem is the \(k\)-SUM problem where \(k=3\).
Moreover, there is a simple reduction from \(k\)-SUM to 2SUM when \(k\) is even
and to 3SUM when \(k\) is odd. Those reductions yield a
\(O(n^{\frac{k}{2}} \log n)\) time real-RAM algorithm for \(k\) even and a
\(O(n^{\frac{k+1}{2}})\) time real-RAM algorithm for \(k\) odd.

In their paper, in addition to the slightly subquadratic uniform algorithm for
3SUM, Gr\o nlund and Pettie give a strongly subquadratic nonuniform
algorithm for 3SUM. The algorithms runs in time \(\tilde{O}(n^{1.5})\), and,
because of the aforementioned reduction, immediately yields an improved
\(\tilde{O}(n^{\frac{k}{2}})\) nonuniform time complexity for \(k\)-SUM when
\(k\) is odd.

As for uniform time complexity we do not know whether this nonuniform
improvement can be transferred to the real-RAM model: we do not know of any
real-RAM \(o(n^{\frac{k+1}{2}})\) time algorithm for \(k\)-SUM when
\(k\) is odd.

The \(k\)-SUM problem reduces to the following point location problem: ``Given
a input point \(q \in \mathbb{R}^n\), locate \(q\) in the arrangement of
\(n \choose k\) hyperplanes of equation \(x_{i_1} + x_{i_2} + \cdots +
x_{i_k} = 0\).'' Applying the best nonuniform algorithms for point location in
arrangements of hyperplanes by Meyer auf der Heide~\cite{M84} and
Meiser~\cite{M93} yields linear decision trees of depth \(n^{O(1)}\) for
\(k\)-SUM, where the constant of proportionality in the big-oh does not depend
on \(k\).

\paragraph{Our contribution on \(k\)-SUM}

Our first contribution with paper A is a finer analysis of Meiser's algorithm
that shows that the depth of his decision tree when applied to \(k\)-SUM is
actually \(O(n^3 \log^2 n)\).  On top of that, we show how to implement a
variant of this decision tree in the real-RAM model so that its uniform running
time is \(n^{\frac{k}{2} + O(1)}\) while keeping the nonuniform running time
unchanged. Note that a naive implementation of Meiser's algorithm has a uniform
running time of \(n^{k + O(1)}\). Those contributions are described in detail
in Chapter~\ref{chapter:ksum-algorithm}.

The approach taken by Meiser's algorithm follows the prune and search method:
Take a sample of the hyperplanes for which we do not yet know the relative
location of the input point. Locate the input point with respect to this sample
by brute-force. This amounts to identifying the cell of the sample's arrangement
which contains the input point. Refine the location of the input point in the
sample by partitioning the containing cell into low complexity subcells. Whenever
this low complexity subcell is located completely on one side of an hyperplane,
the input point is located on the same side, and so we can discard this
hyperplane without a single query to the input point. Since some hyperplanes
may intersect the low complexity subcell, rince and repeat.

The location refinement is necessary because this is the only way we can
guarantee a bound on the number of hyperplanes we have to recurse on.
Using the theory of \(\varepsilon\)-nets, we can show that, for a sample of
reasonable size, any simplex that is not intersected by a sample hyperplane
is not intersected by more than a constant fraction of the set of
hyperplanes from which the sample is drawn (with high probability). We
define the refined subcell of the algorithm presented above to be the simplex
of the bottom-vertex triangulation of the sample's arrangement that contains
the input point.


\paragraph{Sorting}
Before continuing, let us go back to the origins of those different problems.
The link between them will be made even clearer.

Sorting is one of the oldest and most relevant data management problems.
It is the archetypal computation tree problem.
%
Usually presented, sorting is about permutations in \emph{arrays}, but we do
not like that. We use a different abstraction:
%
\input{text/definition/sorting}

In other words, we see the sorting problem as an information retrieval problem:
how many comparisons do we need to make in order to \emph{know} the answer to
all comparisons.
%
The usual sorting algorithms rearrange an array of input numbers into sorted
order. Another way to think about it is that they compute the permutation that
sends the input array to a sorted version of itself. This permutation is a data
structure such that given any index in the input array, we can query for the
corresponding index in the sorted array called the \emph{rank} of the element).
To retrieve the result of a comparison of two elements of the input array we can
compare their ranks in this permutation.
The usual way of defining the sorting problem restricts the data structure that
should be used to encode the \(n \choose 2\) comparisons.
The way we model the sorting problem lifts this restriction because it does not
ask to structure this information in a nice way.

The sorting problem also has its decision problem variant: Element Uniqueness.
%
\input{text/definition/element-uniqueness}

It is easy to see
that Sorting amounts to locating the point \(q\) in
the arrangement of hyperplanes of equations \(x_i - x_j = 0\)%
, and
that Element Uniqueness reduces both to and from the 2SUM
problem in linear time%
.
%
Actually, under reasonable assumptions on the computation model, sorting and
element uniqueness are the same problems: if all questions we ask about the
input are linear in the input numbers, then proving that the input
does not contain any duplicate entries requires to sort the input.
%
The same statement carries over to the \(k\)-SUM with respect to its ``sorting
version'': computing the sign of all \(n \choose k\) sums of \(k\) input
numbers.
%
Therefore, in those models of computation, the sorting problem is equivalent to
2SUM.
%
Because of this relationship between Sorting and the \(k\)-SUM problem,
we see why the better understanding of \(k\)-SUM is a natural next move.

Among all the problems this thesis touches, Sorting and Element Uniqueness
are the best understood. We know \(\Omega(n \log n)\) lower bounds in many
nonuniform models of computation and we also know a long list of simple
real-RAM algorithms for those problems whose running times match those lower
bounds. For all that matters here, those problems are solved.


\paragraph{A Zoo of Generalizations}
Obviously, the \(k\)-SUM problem is not the only possible way to generalize
Sorting.

Hopcroft's problem asks whether given \(n\) points and \(n\) hyperplanes in
\(\mathbb{R}^d\), one of the points lies on one of the hyperplanes. When
\(d=1\), this problem is Element Uniqueness. Finding the location (``above /
below'') of each point
with respect to each hyperplane generalizes Sorting.

The dominance reporting problem asks, given \(n\) points in \(\mathbb{R}^d\),
to report all pairs of points such that the first dominates the other in all
dimensions. Once again, for \(d=1\), this problem is Sorting because it asks
for the answer to all comparisons of the type \(p_i \leq q_i\).

Sorting \(X+Y\) is also a canonical problem in \(P\): given two sets
\(X\) and \( Y \) of \( n \) numbers each, sort the set \( \{\, x + y \colon\,
x \in X, y \in Y\,\} \). Sorting \(X+Y\) reduces linearly to the sorting
version of 4SUM because it asks for the sign of all comparisons of the type
\(x+y \leq x'+y'\).

We already saw that GPT and Sorting belong to the same family of
high-dimensional point location problems. There is a good reason for that: when
\(d=1\), GPT is Element Uniqueness. In one-dimensional space, GPT
asks whether any two points are the same. Picturing this space as
the (horizontal) real line, we see that the ``sorting version'' of GPT asks to
compute for each pair of points which one is on the ``left'' of the other which
simply amounts to Sorting the one-dimensional input points.

%\paragraph{SUBSET-SUM}
%Note that the unparameterized version of \(k\)-SUM is the SUBSET-SUM problem,
%which is NP-complete... Discuss nonuniform polytime algo.

%Because of that, we put Sorting and Element Uniqueness as problems at the
%origin of a zoo of problems.

\paragraph{Intermediate Problems}
A perspicacious reader may have frowned at the previous paragraphs wondering
whether this embryo of classification brings more insight than confusion.
``Sure!'', one may say, ``Many problems involve sorting, and therefore,
according to this dubious definition, are generalizations of it.''

However, the author begs to differ.

As a first example, take one of the best known algorithms for 3SUM. This
algorithm reduces an instance of 3SUM to a sorting phase and a searching
phase~\cite{GP18}. The sorting phase consists in answering all questions of the type
\( x_i + x_{i'} \le x_j + x_{j'} \) with some restriction on the indices.
Well, it
turns out that this phase is exactly an instance of
the sorting version of the 2SUM problem where the input numbers are the
differences \(x_i - x_j\).\footnote{This observation generalizes to the \(k\)-SUM
problem: any \(k\)-SUM instance can be reduced to a larger (\(k-1\))-SUM instance
followed by a searching phase.} Moreover, to make the uniform algorithm
practical, they implement the resolution of the 2SUM instance via dominance
reporting.
Note that this sorting problem is similar to the Sorting \(X+Y\) problem where
the answers to most questions are not cared for.

As a second example, we have the GPT problem. For this problem, the fact that
the one-dimensional version is Sorting does not seem to give any insight on how
to apprehend the more general problem, even in two dimensions. In order to make
some progress in that direction, we need to capture more precisely to which
family of problems GPT belongs. For that, we look at the algebra behind the
problem.

GPT asks whether for any choice of \(n \choose d+1\) input points \(p_i\) with
coordinates \((p_{i,1} , p_{i,2} , \ldots, p_{i,d} )\),
the determinant
%
\begin{displaymath}
  \begin{vmatrix}
    1 & p_{1,1} & p_{1,2} & \hdots & p_{1,d} \\
    1 & p_{2,1} & p_{2,2} & \hdots & p_{2,d} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    1 & p_{d+1,1} & p_{d+1,2} & \hdots & p_{d+1,d}
  \end{vmatrix}
\end{displaymath}
is zero. This determinant is a degree-\(d\) (\(d^2 + d\))-variate polynomial.
In particular, when \(d=2\), the determinant is a degree-\(2\) \(6\)-variate
polynomial. The GPT testing problem then amounts to testing whether the
coordinates of any combination of the input points yields a root of that
polynomial. We therefore consider the more general \(d\)-dimensional-\(k\)-POL
(\(d\)D-\(k\)-POL) problem:
%
Given a \(dk\)-variate constant degree polynomial \(F\) and a set \(S\) of \(n\) points
in \(\mathbb{R}^d\), decide whether \(F(S^k)\) contains any zeroes.
%
For instance, 2D-3POL generalizes GPT with \(d=2\) where the constant degree
polynomial is the \(3 \times 3\) determinant mentioned above. Moreover, 1D-3POL
generalizes 3SUM where the polynomial is simply the sum function. Equally
interesting is the fact that 2D-2POL generalizes Hopcroft's problem with
\(d=2\) where the polynomial is the dot product.

In paper B, we generalize Gr\o nlund and Pettie's approach to solve 1D-3POL (or
more simply, 3POL) in subquadratic time. Our approach is essentially the same
in that it reduces 3POL to a sorting phase and a searching phase, the sorting
phase being an instance of 2D-2POL.\footnote{Note that according to the implied
definition, \(d\)D-\(k\)-SUM is equivalent to \(k\)-SUM. Therefore, the 2SUM
instance in the sorting phase of their 3SUM algorithm is an instance of 2D-2SUM
in disguise.} Again, the implementation of the uniform algorithm solves this
instance of 2D-2POL using dominance reporting (a generalization of it).

This result illustrates why a better understanding of the landscape of problems
surrounding GPT helps to identify intermediate problems whose resolution marks
progress towards the question of whether GPT admits subquadratic algorithms.
\todo{Figure~\ref{XXX} depicts this landscape.}

\paragraph{Encodings}
Naive algorithms for Element Uniqueness, 3SUM, and \(k\)-SUM would search
all possible combinations of \(2\), \(3\), or \(k\) input numbers for a match
and reach horrible running times: respectively \(O(n^2)\), \(O(n^3)\), and \(O(n^k)\).

The way better uniform algorithms for those problems work
is by a combination of sorting and searching. The sorting part
constructs a data structure and the searching part uses this data structure to
answer the question at hand. The achieved running time is a balance between the
cost of sorting and the cost of searching. This results in better
running times than the naive ``search only'' solutions.

For instance, an efficient algorithm for the
Element Uniqueness problem would first sort the input, then scan it for
duplicates among adjacent numbers in this sorted order. The data structure that
is constructed is the permutation we talked about earlier. This structure
achieves a good compressing ratio by encoding the answer to all \(O(n^2)\)
pairwise comparisons of two input numbers in \(O(n \log n)\) bits.

For this reason Element Uniqueness is comparatively simpler than the 3SUM and
\(k\)-SUM problems.
For a nonuniform algorithm, constructing this structure is sufficient to solve
the problem: Once the construction is over, we can discard the input because
all the information we need is encoded in the data structure. Since the nonuniform
models of computation we consider do not care about computations not involving
the input, the searching part does not cost anything.

The case of 3SUM and \(k\)-SUM is more complex: The sorting phase only encodes
a fraction of all possible queries. This leaves a significant amount of work
for the searching phase. Therefore, both the sorting phase and the searching
phase contribute to the nonuniform cost of those algorithms.

The case of GPT is also interesting. The naive algorithm queries
\(O(n^{d+1})\) input tuples while the better algorithm constructs the dual
arrangement in \(O(n^d)\) time.
As in the case of Element Uniqueness,
this dual arrangement is the data structure that encodes the answer to all
\(O(n^{d+1})\) queries while achieving a significant space gain.

While the goal sought in the design of algorithms is to find the best possible
balance between those sorting and searching phases, a different question becomes
evident: ``What is the most resource efficient data structure encoding all the
combinatorial information carried by the input?''. For this question, resource
efficiency can be measured in three ways: space requirements, construction
time, and query time. If one only cares about space, a trivial answer
points its head out: if there are only \(X\) combinatorial types then each type
can be encoded with \(\lceil \log X \rceil\) bits. However, this solution is
unlikely to yield good construction time or good query time.

In the case of Element Uniqueness, this question has been answered long ago.
Sorting the input in \(O(n \log n)\) time will construct a permutation using
\(O(n \log n)\) bits that can be queried for any pairwise comparison in
\(O(1)\) time. However, the question is still widely open for
3SUM, \(k\)-SUM, and GPT.

In paper C we design the first subquadratic space data structure for encoding
the combinatorial type of a two-dimensional GPT instance. This data structure
can be constructed in quadratic time and queries are answered in sublogarithmic
time. Those results can be adapted to work for higher-dimensional GPT to yield
sub-\(O(n^d)\) space data structures with good construction and query times.

Since 3SUM reduces to GPT, the results of paper C can be applied to encode the
combinatorial type of 3SUM instances. However, since 3SUM is much better
understood than GPT we should aim for better encodings. This is exactly what we
do in paper D. By filling the gaps in the partial data structure
used in Gr\o nlund and Pettie's algorithm~\cite{GP18}, we design an encoding
that uses \(O(n^{3/2} \log n)\) bits, can be constructed in \(O(n^2)\) time and
answers queries in constant time.

\todo{Conclude with remark on encodings derived from decision trees.}

\paragraph{The secret ingredient}
\todo{Explain cutting tools and how they allow for fast algorithm.}


\paragraph{Wrapping it up}

Since publication of those papers, a few developments have surfaced.

Ezra and Sharir~\cite{ES17} show how trading simplices of the bottom-vertex
triangulation for prisms of the vertical decomposition in Meiser's algorithm
yields a shallower decision tree of depth \(O(n^2 \log n)\). Essentially, the
improvement over our result in paper A lies in the fact that,
for vertical decomposition, the sample size can be taken to be an order of
magnitude smaller.

%All nonuniform algorithm for \(k\)-SUM we have mentioned query the input with
%\(s\)-linear queries: a \(s\)-linear query asks for the sign of weighted sums of
%\(s\) input numbers. A decision tree that queries the input exclusively with
%\(s\)-linear queries is called a \(s\)-linear decision tree.

%The nonuniform algorithm of Gr\o nlund and Pettie for
%\(k\)-SUM with \(k\) odd uses many more queries than the methods of Meyer auf der
%Heide and Meiser. However, its merit lies in that it only asks very simple
%questions about the input: this algorithm probes the input
%with (\(2k-2\))-linear queries while the others use \(n\)-linear queries.

In a breakthrough paper, Kane, Lovett, and Moran~\cite{KLM18}, give a
\(O(n \log^2 n)\) nonuniform linear decision tree for \(k\)-SUM, almost
matching the \(\Omega(n \log n)\) lower bound. This improves both on paper A
and Ezra and Sharir~\cite{ES17}.

In~\cite{Ch18}, Chan shaves more logarithmic factors from the time complexity
of uniform algorithms for 3SUM and 3POL. While we focused on applications that solve
3SUM-hard geometric problems with one-dimensional input in paper B, he shows how
the ideas that work for 3POL also work for some 3SUM-hard geometric problems
with two-dimensional data.
