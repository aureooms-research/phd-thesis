\chapternonum{To the Profane}

% REMINDER Avoid all technical terms here.

Solving a problem consists in mapping a given data to an output in some
automated way.
In Computer Science, we solve problems with computers: we organize the data
with data structures, and process those structures with algorithms.
%
Solving problems consumes resources, for example, time.
Problems are considered harder if they take more resources to solve.

We, computer scientists, are lazy. We do not want to solve the same problem
over and over with ad-hoc solutions depending on the data.
%
By automated we really mean that the solutions we
design work for any data, and in particular, they work for any data size
``\(n\)''.
%
Ideally, the resource consumption of our solutions should scale well with this
parameter.
%
This is why we study the behaviour of such solutions when applied to large%
\footnote{%
\emph{
	Gigantic multiplied by colossal multiplied by staggeringly
	huge is the sort of concept we're trying to get across here.
}
-- Douglas Adams, The Restaurant at the End of the Universe.
}
inputs.

Geometry is about space, points, curves, surfaces \dots
We study geometric problems:
%
The given data is geometric, and the question about the data is geometric.
The proposed algorithms and data structures therefore exploit geometry.

This thesis studies questions about geometric problems that are simple to
formulate. However simple they are, nobody
has managed to answer them. One of the problems asks to decide, given \(n\) points in the plane,
whether three of them lie on a common line. Of course, we can solve this
problem. The question of interest here is how fast it can be solved.

It is easy to solve this problem by testing all possible candidates but that is
quite inefficient. There is a
more involved but standard construction that takes significantly less time to
execute but that is still considered inefficient. As of today, we do not know of
any reason why this problem would be much harder than simply
reading the data from begin to end.

Does this matter at all? It turns out simple problems appear to
be bottlenecks of more complex ones: unless we manage to improve our
understanding of the simple ones, we are stuck with inefficient solutions for
all of them.

In this thesis, we expose two novel algorithms and two novel data
structures related to the problem stated above. Hopefully, this contribution
will one day, directly or indirectly, help us solve the hard questions about
this problem that have been open for decades.
