\section{[DONE] Algorithms}

We study two classes of algorithms: uniform and nonuniform.

\emph{Uniform} algorithms are considered practical. They can be implemented on
a real computer because they take care of data management issues in an
automated way.

\emph{Nonuniform} algorithms ignore the data management aspect of practical
computation. They do not give details on how to structure the intermediate
results of their computation. Those results are assumed to be instantly
accessible without any organization. Since we consider problem instances of
arbitrarily large size, this is not possible in practice:
processor units can only hold a fixed amount of data at any given time. The
data has to be stored somewhere in some structured way.

The naming uniform simply means that the algorithm (its description) is the
same for all input sizes. This is what is generally expected from algorithm
design: to output a finite size description of an automated problem solving
method that works for any instance size.
%
Nonuniform algorithms on the other hand are allowed to have distinct
descriptions for each input size. Each nonuniform algorithm can be seen as an
infinite sequence of uniform algorithms \(A = A_1, A_2, \ldots\), each \(A_n\)
hardcoding the data management part in its description. This description
therefore is allowed to grow with \(n\).
Designing a nonuniform algorithm \(A\) amounts to describing a method
that given \(n\) outputs \(A_n\).

In this thesis, we study the uniform \emph{random access machine} (RAM) models
real-RAM and word-RAM, and the nonuniform decision tree models algebraic
computation tree, algebraic decision tree, and linear decision tree. Those are
described in the following paragraphs.

\subsection{Random Access Machines}

The real-RAM and word-RAM models have the RAM in common. They both assume
a memory whose access cost is constant and a constant number of standard
operations on the numbers they manipulate.

The real-RAM model is a classic of Computational Geometry. The input is assumed
to consist of \(n\) real numbers and those numbers can be manipulated
exactly at constant cost using arithmetic and comparison operators.
This makes sense from the geometer's point of view since geometric data is
best abstracted as real numbers.
For data management purposes, the model also allows to manipulate \(\log
n\)-bits integers with arithmetic, comparison, and bitwise operators but does
not allow the conversion from a real number to an integer.

The word-RAM model models computers as we know them more closely. It considers
an input consisting of \(n\) integers called words and allows a constant number
of standard unary or binary operators to be executed in constant time. Those
words have bitsize \(w \geq \log n\).

\subsection{Computation and Decision Trees}

A decision tree is a constant-degree rooted tree whose internal nodes encode
queries to an omniscient oracle and whose leaves encode the result of the
computation.

Of course, for every problem asking to distinguish among \(f(n)\) input
classes, there is a decision tree that will ask \(O(\log f(n))\)
``yes/no'' questions of
the form: ``Is the input in the following range of input classes?''
The queries this decision tree makes are too powerful, they have no
structure a priori and it is not at all obvious how they can be encoded.
For a decision tree model to make sense, we have to restrict the kind of
queries that can be made.

The complexity of a computation or decision tree is defined to be the length of
its longest root to leaf path called the \emph{depth} of the tree.

\subsubsection{Linear Decision Trees}

A first natural way to restrict the complexity of the queries of our decision
tree model is to only allow linear queries on the input. This will be
sufficient to solve problems such as Sorting, Element Uniqueness, 3SUM,
\(k\)-SUM, and subset-sum.

In the \emph{\(s\)-linear decision tree model}, queries consists
in testing the sign of a linear function on at most \(s\) numbers \(q_{i_1},\ldots,q_{i_s}\) of the
input \(q_1,\ldots,q_n\). Such a query is called a \(s\)-linear query and
can be written as
%
\begin{displaymath}
	\alpha_1 q_{i_1} + \cdots + \alpha_s q_{i_s} \ask{\leq} \alpha_0
\end{displaymath}
%
Each question is defined to cost a single unit. All other operations can be
carried out for free but may not examine the input vector $q$. We refer to
$n$-linear decision trees simply as linear decision trees.

Note that this model generalizes the Comparison Tree Model commonly used for
Sorting and Element Uniqueness, where queries have the form \(q_i \ask{\leq}
q_j\).

\subsubsection{Algebraic Decision Trees}
The linear decision tree model is not powerful enough for more complicated
problems. General Position Testing for instance involves discovering the sign
of quadratic polynomials of the input, which is impossible in general with
linear queries only.

The (bounded-degree) \emph{algebraic decision tree (ADT)}~\cite{R72,Y81,SY82}
is an algebraic generalization of linear decision trees.
An algebraic decision tree performs constant-cost branching operations that
amount to test the sign of
a constant-degree polynomial of the input numbers.

In this model, for an input \(q_1,\ldots,q_n\), a query can be written as
\begin{displaymath}
	p(q_1, \ldots, q_n) \ask{\le} 0,
\end{displaymath}
where \(p \in \mathbb{R}[x_1,\ldots, x_n]\) is a polynomial of constant
degree. Again, operations not involving the input are free.

\subsubsection{Algebraic Computation Trees}
Computation trees differ from decision trees in that instead of asking
``yes/no'' questions, they perform arithmetic operations on variables whose
results can be remembered and reused as operands, but only inspected using
comparison queries. This allows, for instance, to make decision based on
arbitrary polynomial expressions of the input with a sane way of counting the
complexity of constructing such an expression. This would not be allowed in
the algebraic decision tree model.

The internal nodes of an \emph{algebraic computation trees} are labeled with
arithmetic (\(r \gets o_1 \op o_2, \op \in \{\,+,-,\times,\div\,\}\)) and
branching (\(z : 0\)) operations. Any internal node labeled \(r \gets o_1 \op
o_2\) has outdegree one and is
such that either \(o_k = q_i\) for some \(i\) or there exists an ancestor \(o_k
\gets x \op y\) of this node, and any internal node labeled \(z : 0\) has
outdegree three and is such that either \(z = q_i\) for some \(i\) or there
exists an ancestor \(z \gets x \op y\) of this node.
Similarly to the previously described decision tree models, leafs of the tree
correspond to input classes.

Using this model can be interpreted as only counting the operations that
involve the input, that is, members of the input or results of previous
operations involving the input. All other arithmetic operations are for free.

